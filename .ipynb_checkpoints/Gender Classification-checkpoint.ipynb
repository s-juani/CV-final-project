{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9c9aa3-76d6-470c-9982-f8d90e4a8f31",
   "metadata": {},
   "source": [
    "# <n><u>PARTE 3:</u></n> Deteccion de Genero\n",
    "## Florencia Migues & Santiago Juani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf21c8b-5c03-4c8a-873a-d740339ac7cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497f6273-ef9b-417e-b61b-9950869cbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85e2bc8-3a29-40a4-b6d5-2cc1f07f3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b3dbce-b485-4941-8032-c3734408b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11b319-08a8-42d1-a62e-9f3b560d0af8",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f324d9-b359-4f38-85d6-132dd2182ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_image(img):\n",
    "    if len(img.shape)==3 and img.shape[2]==3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return img/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2f922c-c7aa-4487-90a8-6f49924abfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_training_data(training_data_path, training_labels_path):\n",
    "    \n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    images = sorted(glob(training_data_path + '/*'))\n",
    "    labels = pd.read_pickle(os.path.join(training_labels_path, 'training_labels.pkl'))\n",
    "    \n",
    "    print(f'##Loading {len(images)} face images:', flush=True)\n",
    "    for img_file in tqdm(images, total=len(images)):\n",
    "        image_name = os.path.basename(img_file)\n",
    "        \n",
    "        image = cv2.imread(img_file)[...,::-1]\n",
    "        image = cv2.resize(image, [128, 128], cv2.INTER_AREA)\n",
    "        training_data.append(prep_image(image))\n",
    "        \n",
    "        label = labels.loc[labels[\"image_name\"] == image_name]\n",
    "        #training_labels.append(label['Male'].values[0])\n",
    "        \n",
    "        if label['Male'].values[0] == 'male':\n",
    "            training_labels.append([1,0])\n",
    "        else:\n",
    "            training_labels.append([0,1])\n",
    "        \n",
    "    training_data = np.asarray(training_data)\n",
    "    training_labels = np.asarray(training_labels)\n",
    "    return training_data, training_labels\n",
    "\n",
    "\n",
    "def load_validation_data(validation_data_path, validation_labels_path):\n",
    "    \n",
    "    val_imgs = []\n",
    "    val_labels = []\n",
    "    \n",
    "    images = sorted(glob(validation_data_path + '/*.jpg')) \n",
    "    \n",
    "    bbox_pkl = pd.read_pickle(os.path.join(validation_data_path,'validation_bbox.pickle'))\n",
    "    labels = pd.read_pickle(os.path.join(validation_labels_path, 'testing_labels.pkl'))\n",
    "    \n",
    "    labels.describe()\n",
    "    \n",
    "    print(f'## Loading {len(images)} face images:', flush=True)\n",
    "    for img_file in tqdm(images, total=len(images)):\n",
    "        image_name = os.path.basename(img_file)\n",
    "        \n",
    "        big_image = prep_image(cv2.imread(img_file)[...,::-1])\n",
    "        \n",
    "        bbox = bbox_pkl.loc[bbox_pkl[\"image_id\"]==image_name]\n",
    "        cut_image = big_image[bbox['y_top'].values[0] : bbox['y_top'].values[0]+bbox['height'].values[0],\n",
    "                              bbox['x_left'].values[0] : bbox['x_left'].values[0]+bbox['width'].values[0]]\n",
    "        try:\n",
    "            cut_image = cv2.resize(cut_image, [128, 128], cv2.INTER_AREA)\n",
    "            val_imgs.append(cut_image)\n",
    "            label = labels.loc[labels[\"image_name\"] == image_name]\n",
    "            #val_labels.append(label['Male'].values[0])\n",
    "\n",
    "            if label['Male'].values[0] == 'male':\n",
    "                val_labels.append([1,0])\n",
    "            else:\n",
    "                val_labels.append([0,1])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    val_imgs = np.asarray(val_imgs)\n",
    "    val_labels = np.asarray(val_labels)\n",
    "    return val_imgs, val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31592935-ff33-4b11-9f8e-2d2b37cd7cc3",
   "metadata": {},
   "source": [
    "## Ubicacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce78eb6-dca9-4420-b567-395a0e2d5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "training_data_dir = os.path.join(data_dir, 'face_detection/cropped_faces')\n",
    "training_labels_dir = os.path.join(data_dir, 'gender_labels')\n",
    "\n",
    "validation_data_dir = os.path.join(data_dir, 'face_detection/testing')\n",
    "validation_labels_dir = os.path.join(data_dir, 'gender_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f643d-63e2-4f1f-9fe4-76873374992d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8e8944-86cf-442a-946b-19e47fde1de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Loading 9914 face images:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9914/9914 [00:21<00:00, 467.85it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels = load_training_data(training_data_dir,\n",
    "                                                    training_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348397a5-cb55-4c77-be45-ff6641c0c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loading 175 face images:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 283.37it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_data, validation_labels = load_validation_data(validation_data_dir,\n",
    "                                                          validation_labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbcd7a-f779-4699-8215-74cb9d055f71",
   "metadata": {},
   "source": [
    "## Modelo: Keras CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96b362-7975-4d1e-a28f-5fc62e6ecc7b",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac10174-3f20-478e-ac3e-e7b144cd3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), input_shape=(128, 128, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e3f84-8201-4db1-b7c5-4b159ab6bf35",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e9391-594e-4fd9-969c-e6ce43851bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc18741-cb0b-4589-8a52-0bda634ec9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "check_point_model = os.path.join('./checkpoints','checkpoint_gender_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6966b00-0508-45c0-a042-584e085a6a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('./logs'):\n",
    "    shutil.rmtree('./logs')\n",
    "    \n",
    "checkpoint = ModelCheckpoint(check_point_model,\n",
    "                            monitor='val_accuracy',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "history = model.fit(training_data, training_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(validation_data, validation_labels),\n",
    "                    callbacks=[checkpoint, TensorBoard(f'./logs/gender_model')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abb430-ad0b-48c8-a890-1b6f2e10144d",
   "metadata": {},
   "source": [
    "## Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94507a57-183e-4942-8e49-f84163b03124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gender_classifier_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e30b68d-27f2-4d5a-86a1-939cf386c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.models.load_model('gender_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797b62c-5de3-4f34-955d-41f29102b13b",
   "metadata": {},
   "source": [
    "## Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52aeb65c-42a0-417f-a0b6-c232a5144f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 1s - loss: 0.1970 - accuracy: 0.9249 - 556ms/epoch - 93ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = classifier.evaluate(validation_data,  validation_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c294bd2a-7be3-4223-b1e6-3c36ab767a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.48554706573486%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {test_acc*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed98e1b-e440-4c1f-81e8-37488a6ec9b5",
   "metadata": {},
   "source": [
    "## <n>Accuracy</n> 92.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866555c2-51c1-430b-9fc0-a1081158a0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
