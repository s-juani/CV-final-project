{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0857d980",
   "metadata": {},
   "source": [
    "# Parte 2: Face Detection\n",
    "### Santiago Juani & Florencia Migues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24541589",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920e799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "import os\n",
    "import sklearn\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation import precision_and_recall, interpolated_average_precision, evaluate_detector\n",
    "import sys\n",
    "from image_utils import non_max_suppression\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import shutil\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5362f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractors(Enum):\n",
    "    MiniImage = 1\n",
    "    HOG = 2\n",
    "    LBP = 3\n",
    "    CNN = 4\n",
    "\n",
    "def extract_features(method, image):\n",
    "\t'''Switch between Feature extraction Methods'''\n",
    "\n",
    "\timage_representation = []\n",
    "\n",
    "\tif method == FeatureExtractors.MiniImage:\n",
    "\t\timage_representation = extract_mini_image_features(image)\n",
    "\telif method == FeatureExtractors.HOG:\n",
    "\t\timage_representation = extract_hog_features(image)\n",
    "\telif method == FeatureExtractors.LBP:\n",
    "\t\timage_representation = extract_lbp_features(image)\n",
    "\telif method == FeatureExtractors.CNN:\n",
    "\t\timage_representation = prep_for_cnn(image)\n",
    "\t\n",
    "\treturn image_representation\n",
    "\n",
    "def extract_mini_image_features(image,resize_size=(64,64)):\n",
    "    shape = image.shape\n",
    "    if len(shape) > 2:\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(image,resize_size)\n",
    "    image_representation = resized_image.reshape(resize_size[0]*resize_size[1])\n",
    "    return image_representation\n",
    "  \n",
    "def extract_lbp_features(img):\n",
    "\n",
    "\t\n",
    "\tmeth = 'uniform'\n",
    "\trad = 3\n",
    "\tn_point = 8 * rad\n",
    "\tlbp_img = local_binary_pattern(img, n_point, rad, meth)\n",
    "\tto_return = np.concatenate(lbp_img, axis=0)\n",
    "\treturn to_return\n",
    "\n",
    "\n",
    "def extract_hog_features(img):\n",
    "\thf = hog(img, orientations=10, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)\n",
    "\treturn hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fd089",
   "metadata": {},
   "source": [
    "## Data Loader & ShowImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ad96d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_training_data(training_positive_dir,trainign_negative_dir,feature_extractor=FeatureExtractors.HOG,target_size=[128,128]):\n",
    "    ''' Function for loading loading training data from positive and negative examples\n",
    "    '''\n",
    "    positive_img_files = sorted(glob(training_positive_dir + '/*'))\n",
    "    negative_img_files = sorted(glob(trainign_negative_dir + '/*'))\n",
    "    positive_img_files = positive_img_files[:5000]\n",
    "    negative_img_files = negative_img_files[:5000]\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    print('##Loading {} positive face images'.format(len(positive_img_files)))\n",
    "    for img in tqdm(positive_img_files, total=len(positive_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(1)\n",
    "    \n",
    "    print('##Loading {} negative face images'.format(len(negative_img_files)))\n",
    "    for img in tqdm(negative_img_files,total=len(negative_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(0)   \n",
    "    \n",
    "    training_data = np.asarray(training_data)\n",
    "    training_labels = np.asarray(training_labels)\n",
    "    return training_data, training_labels\n",
    "\n",
    "\n",
    "\n",
    "def load_validation_data(validation_data_dir):\n",
    "\n",
    "    validation_image_files = sorted(glob(validation_data_dir + '/*.jpg'))\n",
    "    val_images = []\n",
    "    \n",
    "    validation_annotations= pd.read_csv(os.path.join(validation_data_dir,'validation_bbox.csv'))\n",
    "    \n",
    "    print(validation_annotations.shape)\n",
    "    validation_bboxes = []\n",
    "    for img_file in tqdm(validation_image_files,total=len(validation_image_files)):\n",
    "        image = cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        val_images.append(image)\n",
    "        image_name = os.path.basename(img_file)\n",
    "        bbox_info = validation_annotations.loc[validation_annotations[\"image_id\"]==image_name]\n",
    "        bbox = np.array([bbox_info['x_left'].values[0],bbox_info['y_top'].values[0],bbox_info['x_left'].values[0]+bbox_info['width'].values[0],bbox_info['y_top'].values[0]+bbox_info['height'].values[0]])\n",
    "        validation_bboxes.append(bbox)\n",
    "        \n",
    "    return val_images, validation_bboxes\n",
    "\n",
    "\n",
    "\n",
    "def show_image_with_bbox(image,bboxes,gt_bbox,draw_GT=True):\n",
    "    if draw_GT: \n",
    "        cv2.rectangle(image, (gt_bbox[0],gt_bbox[1]), (gt_bbox[2],gt_bbox[3]), (0, 0, 255), 2)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        if len(bbox) == 4:   \n",
    "            top_left = (int(bbox[0]),int(bbox[1]))\n",
    "            bottom_right = (int(bbox[0])+ int(bbox[2]),int(bbox[1])+int(bbox[3]))\n",
    "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920c5e4",
   "metadata": {},
   "source": [
    "## Sliding Window\n",
    "Implementacion propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca19f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_windows(img, window_size, step_size):\n",
    "    [i_rows, i_cols] = img.shape;\n",
    "    w_rows = window_size[1];\n",
    "    w_cols = window_size[0];\n",
    "    \n",
    "    cc = (i_cols + step_size - w_cols)/step_size\n",
    "    cr = (i_rows + step_size - w_rows)/step_size\n",
    "    return int(cc*cr)\n",
    "\n",
    "def sliding_window(img, window_size, scale, step_size):\n",
    "    \n",
    "    scales = [3*(scale/4)]#, 2*(scale/3)]\n",
    "    images = []\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    for s in scales:\n",
    "        width = int(img.shape[1]*s)\n",
    "        heigh = int(img.shape[0]*s)\n",
    "        \n",
    "        image = cv2.resize(img, (width, heigh),\n",
    "                           interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        if (image.shape[0] < window_size[0] or image.shape[1] < window_size[1]):\n",
    "            break\n",
    "        \n",
    "        images.append(image)\n",
    "        ct += total_windows(image, window_size, step_size)\n",
    "       \n",
    "    patches = np.zeros((window_size[0], window_size[1], ct))\n",
    "    bbox_locations = np.zeros((ct, 4))\n",
    "\n",
    "    i=0\n",
    "    err = 0\n",
    "    for image in images:\n",
    "        for y in range(0, image.shape[0], step_size):\n",
    "            for x in range(0, image.shape[1], step_size):\n",
    "                try:\n",
    "                    patches[:,:,i] = image[y:y+window_size[0], x:x+window_size[1]]\n",
    "                    bbox_locations[i,:] = [int(x*(float(img.shape[1])/float(image.shape[1]))), \n",
    "                                           int(y*(float(img.shape[0])/float(image.shape[0]))),\n",
    "                                           int(window_size[0]*(float(img.shape[0])/float(image.shape[0]))),\n",
    "                                           int(window_size[1]*(float(img.shape[1])/float(image.shape[1])))]\n",
    "                    i+= 1\n",
    "                except:\n",
    "                    if i >= ct:\n",
    "                        err += 1\n",
    "    print('i: ', i, 't: ', ct, 'err: ', err)\n",
    "    return patches, bbox_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b2d46",
   "metadata": {},
   "source": [
    "## Test Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66e399",
   "metadata": {},
   "source": [
    "#### Ubicacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb03050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./'\n",
    "face_detection_dir = os.path.join(data_dir, 'face_detection')\n",
    "training_faces_dir = os.path.join(face_detection_dir,'cropped_faces')\n",
    "negative_examples_training_dir = os.path.join(face_detection_dir,'negative_data')\n",
    "validation_faces_dir = os.path.join(face_detection_dir,'validation')\n",
    "validation_raw_faces_dir = os.path.join(face_detection_dir,'val_raw_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56ceca",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfa71a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, training_labels = load_training_data(training_faces_dir,\n",
    "                                                   negative_examples_training_dir,\n",
    "                                                   FeatureExtractors.HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c209b18",
   "metadata": {},
   "source": [
    "#### Entrenar el clasificador knn de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f7939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=8)\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "pickle.dump(knn_classifier, open('./face_detector', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdceb8-0dd5-4068-8c09-e147611eae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(kernel='linear',\n",
    "                         probability=True)\n",
    "svm_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12688f",
   "metadata": {},
   "source": [
    "#### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(svm_classifier, open('./face_detector_svm', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc59433",
   "metadata": {},
   "source": [
    "#### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405c9e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "classifier = pickle.load(open('./face_detector_svm','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9d51b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1205bf2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 16/175 [00:00<00:01, 153.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:01<00:00, 151.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  308 t:  331 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  440 t:  469 err:  0\n",
      "i:  45 t:  54 err:  0\n",
      "i:  475 t:  475 err:  616\n",
      "i:  242 t:  247 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  220 t:  233 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  264 t:  281 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  227 t:  227 err:  461\n",
      "i:  396 t:  413 err:  0\n",
      "i:  272 t:  272 err:  492\n",
      "i:  440 t:  466 err:  0\n",
      "i:  440 t:  450 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  264 t:  275 err:  0\n",
      "i:  198 t:  208 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  286 t:  306 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  286 t:  303 err:  0\n",
      "i:  418 t:  447 err:  0\n",
      "i:  198 t:  210 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  220 t:  225 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  101 t:  101 err:  24\n",
      "i:  418 t:  438 err:  0\n",
      "i:  286 t:  312 err:  0\n",
      "i:  227 t:  227 err:  461\n",
      "i:  418 t:  438 err:  0\n",
      "i:  317 t:  317 err:  508\n",
      "i:  308 t:  315 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  198 t:  210 err:  0\n",
      "i:  374 t:  388 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  352 t:  379 err:  0\n",
      "i:  308 t:  329 err:  0\n",
      "i:  80 t:  88 err:  0\n",
      "i:  264 t:  284 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  162 t:  165 err:  0\n",
      "i:  308 t:  329 err:  0\n",
      "i:  497 t:  497 err:  25\n",
      "i:  330 t:  337 err:  0\n",
      "i:  308 t:  329 err:  0\n",
      "i:  198 t:  216 err:  0\n",
      "i:  330 t:  345 err:  0\n",
      "i:  80 t:  86 err:  0\n",
      "i:  430 t:  430 err:  26\n",
      "i:  242 t:  261 err:  0\n",
      "i:  418 t:  444 err:  0\n",
      "i:  220 t:  241 err:  0\n",
      "i:  418 t:  433 err:  0\n",
      "i:  362 t:  362 err:  539\n",
      "i:  220 t:  239 err:  0\n",
      "i:  374 t:  388 err:  0\n",
      "i:  220 t:  230 err:  0\n",
      "i:  227 t:  227 err:  461\n",
      "i:  220 t:  225 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  295 t:  295 err:  492\n",
      "i:  198 t:  202 err:  0\n",
      "i:  352 t:  365 err:  0\n",
      "i:  264 t:  284 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  286 t:  306 err:  0\n",
      "i:  352 t:  379 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  176 t:  196 err:  0\n",
      "i:  286 t:  303 err:  0\n",
      "i:  452 t:  452 err:  26\n",
      "i:  286 t:  312 err:  0\n",
      "i:  144 t:  153 err:  0\n",
      "i:  242 t:  267 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  227 t:  227 err:  461\n",
      "i:  220 t:  225 err:  0\n",
      "i:  63 t:  74 err:  0\n",
      "i:  418 t:  444 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  295 t:  295 err:  492\n",
      "i:  308 t:  315 err:  0\n",
      "i:  220 t:  236 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  198 t:  213 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  242 t:  258 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  242 t:  261 err:  0\n",
      "i:  462 t:  480 err:  0\n",
      "i:  220 t:  244 err:  0\n",
      "i:  440 t:  466 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  330 t:  337 err:  0\n",
      "i:  176 t:  185 err:  0\n",
      "i:  352 t:  371 err:  0\n",
      "i:  242 t:  264 err:  0\n",
      "i:  308 t:  329 err:  0\n",
      "i:  362 t:  362 err:  539\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  308 t:  326 err:  0\n",
      "i:  242 t:  264 err:  0\n",
      "i:  396 t:  421 err:  0\n",
      "i:  286 t:  303 err:  0\n",
      "i:  242 t:  261 err:  0\n",
      "i:  308 t:  334 err:  0\n",
      "i:  308 t:  320 err:  0\n",
      "i:  396 t:  405 err:  0\n",
      "i:  295 t:  295 err:  492\n",
      "i:  154 t:  168 err:  0\n",
      "i:  264 t:  275 err:  0\n",
      "i:  80 t:  83 err:  0\n",
      "i:  198 t:  210 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  198 t:  219 err:  0\n",
      "i:  484 t:  506 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  176 t:  188 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  286 t:  312 err:  0\n",
      "i:  308 t:  329 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  264 t:  270 err:  0\n",
      "i:  286 t:  303 err:  0\n",
      "i:  220 t:  236 err:  0\n",
      "i:  295 t:  295 err:  492\n",
      "i:  264 t:  275 err:  0\n",
      "i:  27 t:  37 err:  0\n",
      "i:  418 t:  435 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  330 t:  354 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  484 t:  506 err:  0\n",
      "i:  286 t:  303 err:  0\n",
      "i:  484 t:  506 err:  0\n",
      "i:  132 t:  135 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  440 t:  464 err:  0\n",
      "i:  295 t:  295 err:  29\n",
      "i:  220 t:  225 err:  0\n",
      "i:  220 t:  244 err:  0\n",
      "i:  198 t:  222 err:  0\n",
      "i:  250 t:  250 err:  476\n",
      "i:  220 t:  225 err:  0\n",
      "i:  242 t:  253 err:  0\n",
      "i:  272 t:  272 err:  492\n",
      "i:  242 t:  258 err:  0\n",
      "i:  264 t:  281 err:  0\n",
      "i:  308 t:  326 err:  0\n",
      "i:  220 t:  225 err:  0\n",
      "i:  264 t:  278 err:  0\n",
      "i:  176 t:  180 err:  0\n",
      "i:  352 t:  371 err:  0\n",
      "i:  352 t:  368 err:  0\n",
      "i:  69 t:  69 err:  390\n",
      "i:  242 t:  253 err:  0\n",
      "i:  220 t:  239 err:  0\n",
      "i:  462 t:  492 err:  0\n",
      "i:  378 t:  378 err:  561\n",
      "i:  308 t:  329 err:  0\n",
      "i:  242 t:  253 err:  0\n",
      "i:  220 t:  225 err:  0\n"
     ]
    }
   ],
   "source": [
    "total_true_positives = []\n",
    "total_real_positives = []\n",
    "total_positive_predictions = []\n",
    "window_size = [128, 128]\n",
    "validation_data, validation_bboxes = load_validation_data(validation_faces_dir)\n",
    "k = 0\n",
    "stride = 8\n",
    "scale = 1\n",
    "\n",
    "for img, gt_bbox in zip(validation_data,validation_bboxes):\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    patches, bbox_locations = sliding_window(gray_image,window_size,scale,stride)\n",
    "    \n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.HOG, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    \n",
    "    ## Get score for each sliding window patch\n",
    "    scores = classifier.predict_proba(patches_feature_representation)  \n",
    "\n",
    "    ## Positive Face Probabilities\n",
    "    face_probabilities = scores[:,1]\n",
    "\n",
    "    [ detected_true_positives, image_real_positives, detected_faces ] = evaluate_detector(bbox_locations, face_probabilities, gt_bbox)\n",
    "    total_true_positives.append(detected_true_positives)\n",
    "    total_real_positives.append(image_real_positives)\n",
    "    total_positive_predictions.append(detected_faces)\n",
    "        \n",
    "total_true_positives = np.asarray(total_true_positives)\n",
    "total_real_positives = np.asarray(total_real_positives)\n",
    "total_positive_predictions = np.asarray(total_positive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18760bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = precision_and_recall(total_true_positives, total_real_positives,total_positive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d894445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0,1.1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41805d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Average Precision is [0.4775391]\n"
     ]
    }
   ],
   "source": [
    "ap = interpolated_average_precision(recall,precision)\n",
    "print('Detection Average Precision is {}'.format(ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c5dd1-125e-4bfc-aa9d-af11a8b1f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd5d548a26ebccf93d004b6c03c44a098cc55bc64ef97095364910cfd10802ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
