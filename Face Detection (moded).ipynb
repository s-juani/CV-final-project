{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750ca177",
   "metadata": {},
   "source": [
    "# Parte 2: Face Detection\n",
    "### Santiago Juani & Florencia Migues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dba216",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c073c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "import os\n",
    "import sklearn\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation import evaluate_detector, precision_and_recall, interpolated_average_precision\n",
    "import sys\n",
    "from image_utils import non_max_suppression\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebeae2e",
   "metadata": {},
   "source": [
    "## Feature Estractors\n",
    "#### Implementacion: Hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7eaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractors(Enum):\n",
    "    MiniImage = 1\n",
    "    HOG = 2\n",
    "    LBP = 3\n",
    "    CNN = 4\n",
    "\n",
    "def extract_features(method, image):\n",
    "\t'''Switch between Feature extraction Methods'''\n",
    "\n",
    "\timage_representation = []\n",
    "\n",
    "\tif method == FeatureExtractors.MiniImage:\n",
    "\t\timage_representation = extract_mini_image_features(image)\n",
    "\telif method == FeatureExtractors.HOG:\n",
    "\t\timage_representation = extract_hog_features(image)\n",
    "\telif method == FeatureExtractors.LBP:\n",
    "\t\timage_representation = extract_lbp_features(image)\n",
    "\telif method == FeatureExtractors.CNN:\n",
    "\t\timage_representation = prep_for_cnn(image)\n",
    "\t\n",
    "\treturn image_representation\n",
    "\n",
    "def extract_mini_image_features(image,resize_size=(64,64)):\n",
    "    shape = image.shape\n",
    "    if len(shape) > 2:\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(image,resize_size)\n",
    "    image_representation = resized_image.reshape(resize_size[0]*resize_size[1])\n",
    "    return image_representation\n",
    "  \n",
    "def extract_lbp_features(img):\n",
    "    return []\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    #img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    fd = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                      cells_per_block=(1, 1), visualize=False)\n",
    "    return fd\n",
    "\n",
    "def prep_for_cnn(img):\n",
    "    #print(img.shape)\n",
    "    if len(img.shape)==3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    return img/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3d26d",
   "metadata": {},
   "source": [
    "## Data Loader & ShowImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(training_positive_dir,trainign_negative_dir,feature_extractor=FeatureExtractors.HOG,target_size=[128,128]):\n",
    "    ''' Function for loading loading training data from positive and negative examples\n",
    "    '''\n",
    "    positive_img_files = sorted(glob(training_positive_dir + '/*'))\n",
    "    negative_img_files = sorted(glob(trainign_negative_dir + '/*'))\n",
    "    positive_img_files = positive_img_files[:7500]\n",
    "    negative_img_files = negative_img_files[:7500]\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    print('##Loading {} positive face images'.format(len(positive_img_files)))\n",
    "    for img in tqdm(positive_img_files, total=len(positive_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(1)\n",
    "    \n",
    "    print('##Loading {} negative face images'.format(len(negative_img_files)))\n",
    "    for img in tqdm(negative_img_files,total=len(negative_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(0)   \n",
    "    \n",
    "    training_data = np.asarray(training_data)\n",
    "    training_labels = np.asarray(training_labels)\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_validation_data(validation_data_dir):\n",
    "\n",
    "    validation_image_files = sorted(glob(validation_data_dir + '/*.jpg'))\n",
    "    val_images = []\n",
    "    validation_annotations= pd.read_pickle(os.path.join(validation_data_dir,'validation_bbox.pickle'))\n",
    "    print(validation_annotations.shape)\n",
    "    validation_bboxes = []\n",
    "    for img_file in tqdm(validation_image_files,total=len(validation_image_files)):\n",
    "        image = cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        val_images.append(image)\n",
    "        image_name = os.path.basename(img_file)\n",
    "        bbox_info = validation_annotations.loc[validation_annotations[\"image_id\"]==image_name]\n",
    "        bbox = np.array([bbox_info['x_left'].values[0],bbox_info['y_top'].values[0],bbox_info['x_left'].values[0]+bbox_info['width'].values[0],bbox_info['y_top'].values[0]+bbox_info['height'].values[0]])\n",
    "        validation_bboxes.append(bbox)\n",
    "        \n",
    "    return val_images, validation_bboxes\n",
    "\n",
    "def show_image_with_bbox(image,bboxes,gt_bbox,draw_GT=True):\n",
    "    if draw_GT: \n",
    "        cv2.rectangle(image, (gt_bbox[0],gt_bbox[1]), (gt_bbox[2],gt_bbox[3]), (0, 0, 255), 2)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        if len(bbox) == 4:   \n",
    "            top_left = (int(bbox[0]),int(bbox[1]))\n",
    "            bottom_right = (int(bbox[0])+ int(bbox[2]),int(bbox[1])+int(bbox[3]))\n",
    "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59613fbe",
   "metadata": {},
   "source": [
    "## Sliding Window\n",
    "Implementacion propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02c2bb-7554-4826-b62c-2b0f77457de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_windows(img, window_size, step_size):\n",
    "    [i_rows, i_cols] = img.shape;\n",
    "    w_rows = window_size[0];\n",
    "    w_cols = window_size[1];\n",
    "    \n",
    "    cc = (i_cols - w_cols)//step_size\n",
    "    cr = (i_rows - w_rows)//step_size\n",
    "    return cc*cr\n",
    "\n",
    "def sliding_window(img, window_size, scale, step_size):\n",
    "    \n",
    "    scales = [scale, 3*(scale/4), scale/2, scale/3]\n",
    "    images = []\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    for s in scales:\n",
    "        width = int(img.shape[1]*s)\n",
    "        heigh = int(img.shape[0]*s)\n",
    "        \n",
    "        image = cv2.resize(img, (width, heigh),\n",
    "                           interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        if (image.shape[0] < window_size[0] or image.shape[1] < window_size[1]):\n",
    "            break\n",
    "        \n",
    "        images.append(image)\n",
    "        ct += total_windows(image, window_size, step_size)\n",
    "       \n",
    "    \n",
    "    patches = np.zeros((window_size[0], window_size[1], 250))\n",
    "    bbox_locations = np.zeros((250, 4))\n",
    "    \n",
    "    i=0\n",
    "    for image in images:\n",
    "        #print(image.shape)\n",
    "        for y in range(0, image.shape[0], step_size):\n",
    "            for x in range(0, image.shape[1], step_size):\n",
    "                try:\n",
    "                    patches[:,:,i] = image[y:y+window_size[0], x:x+window_size[1]]\n",
    "                    bbox_locations[i,:] = [x, y,\n",
    "                                           int(window_size[0]*(float(img.shape[0])/float(image.shape[0]))),\n",
    "                                           int(window_size[1]*(float(img.shape[1])/float(image.shape[1])))]\n",
    "                    i+= 1\n",
    "                    \n",
    "                except:\n",
    "                    #print()\n",
    "                    #print([y, y+window_size[0], x, x+window_size[1]])\n",
    "                    pass\n",
    "    #print('ct: ', ct, \"i: \", i)\n",
    "    return patches, bbox_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f0e20-6869-4dfa-b187-d23905bab4a4",
   "metadata": {},
   "source": [
    "## Test Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c6615-a3f9-4869-aada-2a34723b6d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir='./'\n",
    "face_detection_dir = os.path.join(data_dir, 'face_detection')\n",
    "validation_raw_faces_dir = os.path.join(face_detection_dir,'val_raw_images')\n",
    "\n",
    "val_data, val_bboxes = load_validation_data(validation_faces_dir)\n",
    "\n",
    "image = val_data[100]\n",
    "gray_image = extract_features(FeatureExtractors.CNN, image)\n",
    "\n",
    "patches, bbox_locations = [], []\n",
    "patches, bbox_locations = sliding_window(gray_image,(100, 100), 1, 40)\n",
    "show_image_with_bbox(image, bbox_locations,\n",
    "                     [0,0,0,0],\n",
    "                     draw_GT=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f65e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entrenamiento del Modelo\n",
    "\n",
    "## Carga de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be2362",
   "metadata": {},
   "source": [
    "#### Ubicacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45eb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./'\n",
    "face_detection_dir = os.path.join(data_dir, 'face_detection')\n",
    "training_faces_dir = os.path.join(face_detection_dir,'cropped_faces')\n",
    "negative_examples_training_dir = os.path.join(face_detection_dir,'negative_data')\n",
    "validation_faces_dir = os.path.join(face_detection_dir,'validation')\n",
    "validation_raw_faces_dir = os.path.join(face_detection_dir,'val_raw_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5cc2f",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e4d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data, training_labels = load_training_data(training_faces_dir,\n",
    "                                                   negative_examples_training_dir,\n",
    "                                                   FeatureExtractors.CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235834f",
   "metadata": {},
   "source": [
    "#### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecadd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data = load_validation_data(validation_faces_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b54560",
   "metadata": {},
   "source": [
    "#### Entrenar el clasificador SVM de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71035a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(kernel='linear',\n",
    "                         probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0650575",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011400f-06e9-493f-b1b4-526a36623a8d",
   "metadata": {},
   "source": [
    "## Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6487ec2-727b-4035-b2cd-de12e0a0448e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), input_shape=(128, 128, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20981b-e2cb-4438-9785-853b0f1fd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(training_data,\n",
    "                                                    training_labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de00e9b-26a1-4ecd-ac0c-57994d60bee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9733b",
   "metadata": {},
   "source": [
    "#### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2894729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "pickle.dump(svm_classifier, open('./face_detector', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn\n",
    "model.save('face_detector20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8c380",
   "metadata": {},
   "source": [
    "#### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb4e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svm\n",
    "classifier = pickle.load(open('./face_detector','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn\n",
    "classifier = keras.models.load_model('face_detector30.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a835e",
   "metadata": {},
   "source": [
    "## Sliding Window & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1e88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Afinar parametros:\n",
    "    threshold_p\n",
    "    overlap_threshold\n",
    "    non_max_threshold\n",
    "'''\n",
    "\n",
    "non_max_threshold = 0.8\n",
    "window_size = (128, 128)\n",
    "scale = 1\n",
    "predictions = []\n",
    "threshold_p = 0.5\n",
    "overlap_threshold = 0.5\n",
    "validation_data, validation_bboxes = load_validation_data(validation_faces_dir)\n",
    "sample_images = 15\n",
    "stride = 32\n",
    "for img, gt_bbox in zip(validation_data[:sample_images],validation_bboxes[:sample_images]):\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    patches, bbox_locations = sliding_window(gray_image,window_size,scale,stride)\n",
    "    \n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.CNN, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    \n",
    "    ## Get prediction label for each sliding window patch\n",
    "    if patches_feature_representation.shape[0] > 0:\n",
    "        scores = classifier.predict(patches_feature_representation)\n",
    "        face_probabilities = scores[:,0]\n",
    "        face_bboxes = bbox_locations[face_probabilities>threshold_p]\n",
    "        face_bboxes_probabilites = face_probabilities[face_probabilities>threshold_p]\n",
    "        \n",
    "        # Do non max suppression and select strongest probability box\n",
    "        [selected_bbox, selected_score] = non_max_suppression(face_bboxes,face_bboxes_probabilites,0.1)\n",
    "    else:\n",
    "        selected_bbox = []\n",
    "    show_image_with_bbox(img, selected_bbox,gt_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde708aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25102252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_true_positives = []\n",
    "total_real_positives = []\n",
    "total_positive_predictions = []\n",
    "window_size = [128, 128]\n",
    "validation_data, validation_bboxes = load_validation_data(validation_faces_dir)\n",
    "k = 0\n",
    "stride = 8\n",
    "for img, gt_bbox in zip(validation_data,validation_bboxes):\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    patches, bbox_locations = sliding_window(gray_image,window_size,scale,stride)\n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.CNN, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    ## Get score for each sliding window patch\n",
    "    scores = classifier.predict(patches_feature_representation)  \n",
    "    #print(scores)\n",
    "    ## Positive Face Probabilities\n",
    "    face_probabilities = scores[:,0]\n",
    "    ## liblinbear prediction\n",
    "    #[labels, acc, prob] = predict([],patches_feature_representation, clasifier)\n",
    "    #face_probabilities = np.asarray(prob)\n",
    "    #face_probabilities = face_probabilities.T[0]\n",
    "\n",
    "    [ detected_true_positives, image_real_positives, detected_faces ] = evaluate_detector( bbox_locations, face_probabilities);\n",
    "    total_true_positives.append(detected_true_positives)\n",
    "    total_real_positives.append(image_real_positives)\n",
    "    total_positive_predictions.append(detected_faces)\n",
    "        \n",
    "total_true_positives = np.asarray(total_true_positives)\n",
    "total_real_positives = np.asarray(total_real_positives)\n",
    "total_positive_predictions = np.asarray(total_positive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24656239",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = precision_and_recall(total_true_positives, total_real_positives,total_positive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0,1.1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25686c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = interpolated_average_precision(recall,precision)\n",
    "print('Detection Average Precision is {}'.format(ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a502f0-4a40-4e0c-bc4f-98ad1e37086e",
   "metadata": {},
   "source": [
    "### Implementacion SVM:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59dcad5a-3ac7-4406-a64f-53aff2c4ec89",
   "metadata": {},
   "source": [
    "0.11652348\n",
    "0.30308673\n",
    "0.30744853\n",
    "0.31153742"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee470704-cd66-46fb-9ff3-361b393129b5",
   "metadata": {},
   "source": [
    "### Implementacion CNN:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24c163ed",
   "metadata": {},
   "source": [
    "  1 epochs => 0.06067227\n",
    " 10 epochs => 0.16518596\n",
    " 30 epochs => 0.14295763\n",
    "100 epochs => 0.12229414\n",
    "\n",
    "#--- BUGFIX SLIDING WINDOW ---#\n",
    "\n",
    " 30 epochs => 0.29430181"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
